\documentclass[letter]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{ifthen}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}


%%%
% Set up the margins to use a fairly large area of the page
%%%
\oddsidemargin=.2in
\evensidemargin=.2in
\textwidth=6in
\topmargin=-.5in
\textheight=9in
\parskip=.07in
\parindent=0in
\pagestyle{fancy}

%%%
% Set up the header
%%%
\newcommand{\setheader}[6]{
	\lhead{{\sc #1}\\{\sc #2} ({\small \it \today})}
	\rhead{
		{\bf #3} 
		\ifthenelse{\equal{#4}{}}{}{(#4)}\\
		{\bf #5} 
		\ifthenelse{\equal{#6}{}}{}{(#6)}%
	}
}

\makeatletter
\newcommand{\escapeus}{\begingroup\@makeother\_\@escapeus}
\newcommand*{\@escapeus}[1]{#1\endgroup}
\makeatother

%%%
% Set up some shortcut commands
%%%
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Proj}{\mathrm{proj}}
\newcommand{\Perp}{\mathrm{perp}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\Span}{\mathrm{span}}
\newcommand{\Null}{\mathrm{null}}
\newcommand{\Rank}{\mathrm{rank}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\var}[1]{{$\langle$\it #1$\rangle$}}
\newcommand{\Code}[1]{\texttt{\escapeus #1}}

%%%
% This is where the body of the document goes
%%%
\begin{document}
	\setheader{MAT335}{Homework 2}{Due: 11:59pm February 16}{}{}{}

	\begin{enumerate}
		\item 
		McDonalds recently negotiated a large purchasing deal for fish, chicken, and beef.  They have agreed to purchase
		40 million tons of fish, 40 million tons of chicken, and 100 million tons of beef.  As such, they want to create an advertising campaign
		to ensure that consumers eat the correct portion of each meat product.

		After paying to have a commercial produced, McDonalds collects the following data: After watching the commercial once,
		a person who initially wanted fish now has a 10\% chance of buying a fish product, a 60\% chance of buying a beef product,
		and a 30\% chance of buying a chicken product; after watching, a person who initially wanted to buy a beef product has
		a 20\% chance of buying a fish product, a 60\% chance of buying a beef product, and a 20\% chance of buying a chicken product;
		after watching, a person who initially wanted to by a chicken product has a 40\% chance of buying a fish product, a 50\%
		chance of buying a beef product, and a 10\% chance of buying a chicken product.
		\begin{enumerate}
			\item If the vector $\vec e_1$ represents a person who wants 
			to buy a fish product, $\vec e_2$ represents a person who wants 
			to buy a beef product, and $\vec e_3$ represents a person who wants 
			to buy a chicken product, find a matrix $M$ such that $M\vec e_i$ gives the probability of buying fish, beef,
			or chicken after watching the commercial once.

			\item Compute the eigenvalues and eigenvectors of $M$.

			\item Assume that a fish product takes 50 grams
			of fish, a beef product takes 50 grams of beef,
			and a chicken product takes 50 grams of chicken.
			Further, assume that each time a person watches
			the commercial it has the same impact (i.e.,
			watching the commercial twice means the likelihood
			of buying a particular product is given by $M^2$).
			If McDonalds ensures that the average customer
			sees the commercial 3000 times, what are the
			relative proportions of fish, beef, and chicken
			McDonalds expects to sell?

			\item Should McDonalds run the ad? Does the
			initial population's preferences for fish,
			beef, or chicken matter? \emph{Explain your
			reasoning.}
		\end{enumerate}

		\item Throughout this problem, let $P$ be an $n\times n$ \emph{stochastic matrix}. 
		\begin{enumerate}
			\item Prove that if $\vec q$ is a \emph{probability vector}, then $P\vec q$ is also
				a probability vector.
			\item Prove that $P^k$ is a stochastic matrix for all $k\geq 0$.

			\item A \emph{left eigenvector} for $P$ is a non-zero row vector $\vec w$ so
				that $\vec wP=\lambda \vec w$.

				Does $P$ have a left eigenvector with eigenvalue $1$? Why or why not?
			\item Show that the left and right eigenvectors of $P$ may be different but the left
				and right eigenvalues of $P$ must be the same. \emph{Hint: you may use
				facts from linear algebra about determinants and transposes.}

			\item The \emph{unit $n$-symplex} is the set of all convex linear combinations
				of $\vec e_1,\ldots,\vec e_n$. Let $S$ be the unit $n$ symplex, and let $\mathcal P:\R^n\to\R^n$
				be the linear transformation defined by $\mathcal P(\vec x)=P\vec x$. Show that $\mathcal P(S)\subseteq S$.
				\emph{Hint: start by showing that vectors in $S$ are probability vectors.}

			\item The Brouwer Fixed-point Theorem states that a continuous map from a simplex into itself
				has at least one fixed point. Use the Brouwer Fixed-point Theorem to show that $P$ has
				at least one (right) eigenvector with eigenvalue $1$ \emph{which is also a probability vector}.
		\end{enumerate}

		\item We're going to prove some linear algebra facts because, \emph{just maybe} they'll be useful.
		
			Let $P$ be an $n\times n$ stochastic matrix and let $\mathcal P:\R^n\to\R^n$ be the linear
			transformation induced by $P$ (i.e., given by matrix multiplication). Let $S$ be the unit $n$-symplex.
		\begin{enumerate}
			\item Draw $S$ when $n=1$, $2$, and $3$.
			\item Prove that $S$ is equal to the set of all probability vectors in $\R^n$.
			\item Prove that if $\vec p,\vec q\in S$, then all convex linear combinations of $\vec p$
				and $\vec q$ are in $S$.
			\item For the rest of this problem, assume $n\geq 2$.

				The \emph{boundary} of $S$, written $\partial S$, consists of all vectors in $S$ where at least one coordinate
				is zero.

				Let $\vec a,\vec b\in S$ be distinct points and let $\ell\subseteq \R^n$ be the line passing through $\vec a$
				and $\vec b$. Prove that $\ell$ intersects the boundary of $S$.
			\item Prove that if $V\subseteq \R^n$ is a subspace of \emph{dimension at
				least two}, then the following holds: if $V\cap S$ is nonempty, then $V\cap \partial S$ is non-empty.
			\item Prove that if $\vec a,\vec b\in S$ are distinct eigenvectors for $P$ with eigenvalue $1$, then there
				exists a $\vec d\in \partial S$ which is an eigenvector for $P$ with eigenvalue $1$.
		\end{enumerate}

		\item Let $\mathcal M=(M_0,M_1,\ldots)$ be a stationary Markov chain on a graph $\mathcal G$
			with $n$ vertices, and let $P$ be the (stochastic) transition matrix for $\mathcal M$. 
			Further, suppose $\mathcal M$ is modeled by the dynamical system $(T,\Omega)$,
			where $\Omega$ is the space of probability distributions on the $n$ vertices.
		\begin{enumerate}
			\item Produce examples where $\lim_{k\to\infty} P^k$ exists and does not exist. Can you find
				conditions on $\mathcal M$ and $\mathcal G$ so that $\lim_{k\to\infty} P^k$ always exists?
			\item A \emph{stationary distribution} for $\mathcal M$ is defined to be a fixed-point of 
				$(T,\Omega)$. Produce examples where $\mathcal M$ has exactly 1, 2, and 3 stationary distributions.

			\item Prove that a convex linear combination of stationary distributions for $\mathcal M$ 
				is a stationary distribution for $\mathcal M$.

			\item Prove that $\mathcal M$ always has \emph{at least one} stationary distribution.

			\item A Markov chain is called \emph{primitive} if there exists a $k\in \N$ such that
				the probability of transitioning from state $i$ to state $j$ in exactly $k$ steps
				is positive \emph{for every $i$ and $j$}.

				\smallskip
				A distribution $\vec d\in \Omega$ is said to have \emph{full support} if none of the entries
				in $\vec d$ are zero.

				\smallskip
				Show that if $\mathcal M$ is primitive, then every stationary distribution for $\mathcal M$
				must have full support.
			\item Prove that if $\mathcal M$ is primitive, then $\mathcal M$ has a \emph{unique} stationary distribution.
			\item Show that if $\mathcal M$ is primitive and $\lim_{k\to\infty} P^k=P'$ exists, then $P'=[\vec s|\vec s|\cdots|\vec s]$,
				where $\vec s$ is the unique stationary distribution for $\mathcal M$.
		\end{enumerate}


	\end{enumerate}


	\subsection*{Programming Problems}
	For the programming problems, please use the Jupyter notebook available at

	\url{https://utoronto.syzygy.ca/jupyter/user-redirect/git-pull?repo=https://github.com/siefkenj/2020-MAT-335-webpage&subPath=homework/homework2-exercises.ipynb}

	Make sure to comment your code and use ``Markdown'' style cells to explain your answers.

	\begin{enumerate}
		\item {\tt np.random.rand()} will generate a random number chosen uniformly from the interval $[0,1]$. That
			means, if $x,y\in [0,1]$ and $x<y$, the probability that the output of {\tt np.random.rand()} lies in $[x,y]$
			is $y-x$.

		\begin{enumerate}
			\item 
			Using {\tt np.random.rand()}, create a function \verb|pick_random| which inputs a list of the form
			{\tt [(\var{probability}, \var{state}),...]} and returns one of the states chosen at random (but with 
			the appropriate probability).
			\item Create a function \verb|pick_random_n| which inputs a distribution and a number $n$ and outputs $n$
				\emph{samples} from that distribution.
				
				Using the code provided, plot the theoretical (exact) distribution and the empirical (simulated)
				distribution.
		\end{enumerate}

		\item A Python \emph{dictionary} is an like an array except that it can be indexed by things other than numbers\footnote{
		In general, such an object is called an \emph{associative array}.}. It's written with curly braces, colons, and commas.
		For instance, writing \verb|x = {"a": 4, "b": 100}| sets {\tt x} to be a dictionary with keys {\tt "a"} and {\tt "b"}.
		Executing \verb|x["a"]| will return {\tt 4} and \verb|x["b"]| will return {\tt 100}.

		Dictionaries are an ideal object for storing \emph{graphs} and consequently Markov chains. We will store a Markov chain
		in {\tt \var{state}: \var{distribution}} pairs listed in a dictionary.
		\begin{enumerate}
			\item Create a function {\tt step(\var{starting state}, \var{chain})} that inputs a starting state and a Markov chain
				and outputs the results of going one step along the chain.
			\item Create a function \verb|n_orbit| that takes a starting state, Markov chain, and a number of steps $n$, and
				returns the $n$-step realization of that Markov chain.
			\item The {\tt Counter} function will input a list and output a dictionary with the count of each item in that list.
				Using {\tt Counter}, create a \verb|make_dist| function which inputs a sequence of states and outputs
				the empirical distribution coming from those states\footnote{ The \emph{empirical} distribution
				of a sequence of states is the relative proportions of those states in the sequence.}. Make sure
				the resulting distribution is \emph{sorted in alphabetical order by state}\footnote{ You may want to look
				up the {\tt sorted} command in Python.}.

				Using \verb|make_dist|, plot the empirical and theoretical steady state distributions for CHAIN1.
		\end{enumerate}

		\item
			\begin{enumerate}
				\item Make a plot showing the empirical distributions arising from realizations of CHAIN2 starting
					at states {\tt "a"}, {\tt "b"}, \ldots, etc.. Does the starting state affect the empirical
					distribution? Is this what you expect?

					The \verb|ensure_consistent_dists()| function can be used to ensure that a list of distributions
					share the same states so that they can be graphed together.
				\item Compute the stationary distribution for CHAIN2\footnote{ You can use {\tt np.linalg.eig}
					to compute eigenvectors and eigenvalues in Python. Read the documentation to figure out exactly
					what the return values are!}. Does this match what you see from the empirical distribution?
				\item Instead of computing an empirical distribution from a single realization. Let's use a bunch of realizations!
					Suppose $r_{100}$ represents the state a step 100 of a realization of CHAIN2. Using at least 10000
					different realizations, plot the empirical distribution of the different $r_{100}$'s. Do
					the same for the $r_{101}$'s. Is this the same
					or different from the stationary distribution for CHAIN2? Is that what you expected?
				\item Repeat (a)-(c) for CHAIN3. Explain your results.
			\end{enumerate}

		\item {\bf Shakespeare vs.~Lawyers}
		\begin{enumerate}
			\item Execute the code which downloads the complete works of Shakespeare and the Ontario Criminal Code.
			\item Create a function \verb|make_chain| which takes in a realization of a Markov chain and outputs
				a dictionary of transition probabilities (in the same format that CHAIN1, CHAIN2, and CHAIN3
				are stored).
			\item Create Markov chains using the complete works of Shakespeare and the Ontario Criminal Code
				as ``realizations''.
			\item Using your Shakespeare and Ontario Criminal Code Markov chains, create length-30 realizations
				with the starting words ``the'', ``sunset'', and ``satisfied''\footnote{ You can use the command
				{\tt " ".join(\var{list})} to create a string consisting of the words in a list.}.
			\item The simple Markov chain's we've made don't pay attention to punctuation or sentence length. Using
				your human judgement, properly format the realizations of your Markov chains (as best you can).
				The sentences might not make complete English sense, but that's okay!
		\end{enumerate}
	\item {\bf Iterated Function Systems}
		\begin{enumerate}
			\item Read and understand the first three cells in the \emph{Iterated Functions} section.
			\item The sample code takes a single realization of the iterated function system, applies
				it to the point $(1,1)$, and plots it.

				Make a plot of the distribution of this iterated function system by plotting the image
				of many points under many different realizations. What shape do you see?
			\item Using brightness, we can visualize distributions more easily. If you set {\tt additive=True}
				in the \verb|render_points_to_array()| function, a value of $1$ will be added for each
				point that lands in a particular pixel. This means the more points that land in a pixel,
				the ``brighter'' it will look.

				Create a new chain, \verb|F_CHAIN2| which is the same as \verb|F_CHAIN| but with a $1/4$ chance
				of transitioning to {\tt f1} and {\tt f2} and a $1/2$ chance of transitioning to {\tt f3} 
				(regardless of state). Plot the distributions of both \verb|F_CHIAN| and \verb|F_CHAIN2|. Are the 
				results what you expect?

				\emph{Hint:} You may find it easier to visualize if you graph log-intensities instead of 
				direct intensities. You could do this by applying {\tt np.log(\var{density array} + 1)}
				on your data before plotting.

			\item Numpy can perform matrix multiplication with the {\tt @} symbol. For example,
				{\tt np.array([[1,2],[3,4]]) @ np.array([1,1])} will multiply the matrix
				$\mat{1&2\\3&4}$ by the vector $\mat{1\\1}$. Many other linear algebra functions
				can be found by typing {\tt np.linalg.\var{tab}}.

				Create a new iterated function system, \verb|ROT_CHAIN|, with the same functions as \verb|F_CHAIN|,
				except that $f_1$ rotates the resulting vector counter-clockwise by $30^\circ$ before returning it,
				and $f_2$ rotates the resulting vector $30^\circ$ clockwise before returning it.

				Plot the resulting distribution. Is it what you expected?

			\item Read about the Barnsley Fern \url{https://en.wikipedia.org/wiki/Barnsley_fern}. Create
				an iterated function system \verb|BARNSLEY_CHAIN| whose maximal invariant set is the Barnsley
				Fern. Graph the maximal invariant set (or the distribution, whichever you please).
				

				
		\end{enumerate}

	\item {\bfseries Flam3} A very popular algorithm for generating fractals is the \emph{flam3} algorithm\footnote{ It is pronounced
		\emph{flame}, like the thing that comes out of a candle.}. The \emph{flam3} algorithm consists of two parts:
		(a) functions to be used as part of an iterated function system, and (b) a rendering algorithm that produces
		smoothed-out graphics with varying color (instead of the pixely-looking fractals that we are rendering).

		Look over the functions used in the \emph{flam3} algorithm here: \url{https://github.com/AlexanderJenke/FractalFlame/blob/master/functions.py}
		Each function accepts an $(x,y)$ pair and a number of parameters which can be set by the user.

		Download and run JWildfire \url{http://www.jwildfire.org} which is a program that will let you interactively
		(and in real time) adjust the parameters of a \emph{flam3} fractal\footnote{ If you're picky about software, you can
		use a different \emph{flam3} renderer.}.

		For this question: \emph{make a pretty fractal!}

		If you want to start with something familiar, download and save \url{https://raw.githubusercontent.com/siefkenj/2020-MAT-335-webpage/master/homework/basic_sierpinksi.flame}
		Then load \verb|basic_sierpinksi.flame| into JWildfire. You will then see three ``linear3D'' functions in the \emph{Transformations} tab.
		Below, in the \emph{Affine} tab, you can see the matrices that correspond to each transformation. If you click one of the triangles
		in the middle section, you can drag them around to interactively change the defining matrix. You can also middle click or right click to
		adjust the parameters in different ways. You can also have lots of fun by playing with the \emph{Nonlinear} tab.





	\end{enumerate}



\end{document}
